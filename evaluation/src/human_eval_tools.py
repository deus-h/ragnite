"""
Human Evaluation Tools

This module provides tools for human evaluation of RAG systems.
"""

import json
import os
import csv
from typing import List, Dict, Any, Union, Optional, Set, Tuple
from datetime import datetime


class HumanEvaluationTemplate:
    """
    A class for creating human evaluation templates.
    """
    
    DEFAULT_CRITERIA = {
        "relevance": {
            "question": "How relevant is the answer to the query?",
            "description": "Assess whether the answer addresses the main points of the query.",
            "scale": [
                {"value": 1, "label": "Not relevant at all"},
                {"value": 2, "label": "Slightly relevant"},
                {"value": 3, "label": "Moderately relevant"},
                {"value": 4, "label": "Very relevant"},
                {"value": 5, "label": "Perfectly relevant"}
            ]
        },
        "accuracy": {
            "question": "How accurate is the information in the answer?",
            "description": "Assess whether the information provided is factually correct.",
            "scale": [
                {"value": 1, "label": "Contains serious errors"},
                {"value": 2, "label": "Contains minor errors"},
                {"value": 3, "label": "Mostly accurate with caveats"},
                {"value": 4, "label": "Accurate with minor issues"},
                {"value": 5, "label": "Completely accurate"}
            ]
        },
        "completeness": {
            "question": "How complete is the answer?",
            "description": "Assess whether the answer covers all aspects of the query.",
            "scale": [
                {"value": 1, "label": "Misses critical information"},
                {"value": 2, "label": "Misses important points"},
                {"value": 3, "label": "Covers main points only"},
                {"value": 4, "label": "Covers most points in detail"},
                {"value": 5, "label": "Comprehensively answers the query"}
            ]
        },
        "coherence": {
            "question": "How coherent and well-structured is the answer?",
            "description": "Assess whether the answer is logically organized and easy to follow.",
            "scale": [
                {"value": 1, "label": "Incoherent or confusing"},
                {"value": 2, "label": "Poorly structured"},
                {"value": 3, "label": "Adequately structured"},
                {"value": 4, "label": "Well-structured"},
                {"value": 5, "label": "Exceptionally clear and coherent"}
            ]
        },
        "helpfulness": {
            "question": "How helpful is the answer?",
            "description": "Assess whether the answer would be useful to someone asking the query.",
            "scale": [
                {"value": 1, "label": "Not helpful at all"},
                {"value": 2, "label": "Slightly helpful"},
                {"value": 3, "label": "Moderately helpful"},
                {"value": 4, "label": "Very helpful"},
                {"value": 5, "label": "Extremely helpful"}
            ]
        }
    }
    
    def __init__(
        self,
        title: str = "RAG System Evaluation",
        description: str = "Please evaluate the following responses generated by the RAG system.",
        criteria: Optional[Dict[str, Dict[str, Any]]] = None,
        include_free_text: bool = True,
        free_text_prompt: str = "Please provide any additional feedback or observations:",
        comparison_mode: bool = False,
    ):
        """
        Initialize a human evaluation template.
        
        Args:
            title: Title of the evaluation form
            description: Description of the evaluation task
            criteria: Dictionary of evaluation criteria (defaults to standard criteria)
            include_free_text: Whether to include a free text field for comments
            free_text_prompt: Prompt for the free text field
            comparison_mode: Whether to enable system comparison mode
        """
        self.title = title
        self.description = description
        self.criteria = criteria or self.DEFAULT_CRITERIA
        self.include_free_text = include_free_text
        self.free_text_prompt = free_text_prompt
        self.comparison_mode = comparison_mode
    
    def to_html(
        self,
        queries: List[str],
        responses: List[str],
        comparison_responses: Optional[List[str]] = None,
        system_a_name: str = "System A",
        system_b_name: str = "System B",
    ) -> str:
        """
        Generate an HTML evaluation form.
        
        Args:
            queries: List of queries
            responses: List of responses to evaluate
            comparison_responses: Optional list of responses to compare against
            system_a_name: Name of the first system (for comparison mode)
            system_b_name: Name of the second system (for comparison mode)
            
        Returns:
            HTML string for the evaluation form
        """
        if self.comparison_mode and not comparison_responses:
            raise ValueError("Comparison mode requires comparison_responses")
        
        if comparison_responses and len(queries) != len(comparison_responses):
            raise ValueError("Number of queries must match number of comparison_responses")
        
        if len(queries) != len(responses):
            raise ValueError("Number of queries must match number of responses")
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>{self.title}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ margin-bottom: 20px; }}
        .evaluation-item {{ margin-bottom: 30px; border: 1px solid #ddd; padding: 20px; border-radius: 5px; }}
        .query {{ font-weight: bold; margin-bottom: 10px; }}
        .response {{ background-color: #f9f9f9; padding: 10px; margin-bottom: 15px; border-radius: 5px; }}
        .criteria {{ margin-bottom: 15px; }}
        .criteria-question {{ font-weight: bold; }}
        .criteria-description {{ font-style: italic; color: #666; }}
        .scale {{ display: flex; justify-content: space-between; margin: 10px 0; }}
        .scale-item {{ text-align: center; }}
        .scale-item label {{ display: block; margin-top: 5px; font-size: 12px; }}
        .free-text {{ width: 100%; height: 80px; margin-top: 10px; }}
        .submit-button {{ margin-top: 20px; padding: 10px 20px; background-color: #4CAF50; color: white; border: none; cursor: pointer; border-radius: 5px; }}
        .comparison {{ display: flex; justify-content: space-between; }}
        .comparison-response {{ width: 48%; }}
        .comparison-label {{ font-weight: bold; margin-bottom: 5px; }}
        .system-preference {{ margin-top: 15px; }}
        .preference-options {{ display: flex; justify-content: space-between; margin-top: 10px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>{self.title}</h1>
        <p>{self.description}</p>
    </div>
    <form id="evaluation-form">
        <input type="hidden" name="timestamp" value="{datetime.now().isoformat()}">
"""
        
        for i, (query, response) in enumerate(zip(queries, responses)):
            html += f"""
        <div class="evaluation-item">
            <div class="query">Query {i+1}: {query}</div>
"""
            
            if self.comparison_mode and comparison_responses:
                html += f"""
            <div class="comparison">
                <div class="comparison-response">
                    <div class="comparison-label">{system_a_name}:</div>
                    <div class="response">{response}</div>
                </div>
                <div class="comparison-response">
                    <div class="comparison-label">{system_b_name}:</div>
                    <div class="response">{comparison_responses[i]}</div>
                </div>
            </div>
            
            <div class="system-preference">
                <div class="criteria-question">Which system's response do you prefer?</div>
                <div class="preference-options">
                    <div>
                        <input type="radio" id="prefer-a-{i}" name="preference-{i}" value="a">
                        <label for="prefer-a-{i}">Strongly prefer {system_a_name}</label>
                    </div>
                    <div>
                        <input type="radio" id="slight-a-{i}" name="preference-{i}" value="slight-a">
                        <label for="slight-a-{i}">Slightly prefer {system_a_name}</label>
                    </div>
                    <div>
                        <input type="radio" id="equal-{i}" name="preference-{i}" value="equal">
                        <label for="equal-{i}">Equal quality</label>
                    </div>
                    <div>
                        <input type="radio" id="slight-b-{i}" name="preference-{i}" value="slight-b">
                        <label for="slight-b-{i}">Slightly prefer {system_b_name}</label>
                    </div>
                    <div>
                        <input type="radio" id="prefer-b-{i}" name="preference-{i}" value="b">
                        <label for="prefer-b-{i}">Strongly prefer {system_b_name}</label>
                    </div>
                </div>
            </div>
"""
            else:
                html += f"""
            <div class="response">{response}</div>
"""
            
            for criterion_id, criterion in self.criteria.items():
                html += f"""
            <div class="criteria">
                <div class="criteria-question">{criterion['question']}</div>
                <div class="criteria-description">{criterion['description']}</div>
                <div class="scale">
"""
                
                for scale_item in criterion['scale']:
                    html += f"""
                    <div class="scale-item">
                        <input type="radio" id="{criterion_id}-{i}-{scale_item['value']}" name="{criterion_id}-{i}" value="{scale_item['value']}">
                        <label for="{criterion_id}-{i}-{scale_item['value']}">{scale_item['label']}</label>
                    </div>
"""
                
                html += """
                </div>
            </div>
"""
            
            if self.include_free_text:
                html += f"""
            <div>
                <label for="comments-{i}">{self.free_text_prompt}</label>
                <textarea id="comments-{i}" name="comments-{i}" class="free-text"></textarea>
            </div>
"""
            
            html += """
        </div>
"""
        
        html += """
        <button type="submit" class="submit-button">Submit Evaluation</button>
    </form>
    <script>
        document.getElementById('evaluation-form').addEventListener('submit', function(e) {
            e.preventDefault();
            const formData = new FormData(this);
            const results = Object.fromEntries(formData.entries());
            
            // Convert results to JSON and display (in a real app, you'd send this to a server)
            const resultsJSON = JSON.stringify(results, null, 2);
            alert('Results submitted:\\n' + resultsJSON);
            
            // In a real application, you would send this data to your server
            // fetch('/submit-evaluation', {
            //     method: 'POST',
            //     headers: { 'Content-Type': 'application/json' },
            //     body: resultsJSON
            // })
            // .then(response => response.json())
            // .then(data => console.log(data))
            // .catch(error => console.error('Error:', error));
        });
    </script>
</body>
</html>
"""
        
        return html
    
    def save_html(
        self,
        filename: str,
        queries: List[str],
        responses: List[str],
        comparison_responses: Optional[List[str]] = None,
        system_a_name: str = "System A",
        system_b_name: str = "System B",
    ) -> str:
        """
        Generate and save an HTML evaluation form.
        
        Args:
            filename: Path to save the HTML file
            queries: List of queries
            responses: List of responses to evaluate
            comparison_responses: Optional list of responses to compare against
            system_a_name: Name of the first system (for comparison mode)
            system_b_name: Name of the second system (for comparison mode)
            
        Returns:
            Path to the saved HTML file
        """
        html = self.to_html(
            queries=queries,
            responses=responses,
            comparison_responses=comparison_responses,
            system_a_name=system_a_name,
            system_b_name=system_b_name,
        )
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html)
        
        return filename


class HumanEvaluationProcessor:
    """
    A class for processing human evaluation results.
    """
    
    def __init__(self):
        """
        Initialize the human evaluation processor.
        """
        pass
    
    def load_results(self, results_file: str) -> List[Dict[str, Any]]:
        """
        Load evaluation results from a file.
        
        Args:
            results_file: Path to the results file (JSON or CSV)
            
        Returns:
            List of evaluation result dictionaries
        """
        ext = os.path.splitext(results_file)[1].lower()
        
        if ext == '.json':
            with open(results_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif ext == '.csv':
            results = []
            with open(results_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    results.append(row)
            return results
        else:
            raise ValueError(f"Unsupported file format: {ext}")
    
    def save_results(self, results: List[Dict[str, Any]], output_file: str) -> None:
        """
        Save evaluation results to a file.
        
        Args:
            results: List of evaluation result dictionaries
            output_file: Path to save the results
        """
        ext = os.path.splitext(output_file)[1].lower()
        
        if ext == '.json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2)
        elif ext == '.csv':
            if not results:
                raise ValueError("No results to save")
            
            fieldnames = results[0].keys()
            with open(output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for result in results:
                    writer.writerow(result)
        else:
            raise ValueError(f"Unsupported file format: {ext}")
    
    def calculate_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Calculate metrics from evaluation results.
        
        Args:
            results: List of evaluation result dictionaries
            
        Returns:
            Dictionary of calculated metrics
        """
        if not results:
            return {}
        
        metrics = {
            "num_evaluators": len(results),
            "criteria_scores": {},
            "system_preferences": {},
            "comments": []
        }
        
        # Process criteria scores
        criteria_keys = [key for key in results[0].keys() if key.startswith("relevance-") or
                                                           key.startswith("accuracy-") or
                                                           key.startswith("completeness-") or
                                                           key.startswith("coherence-") or
                                                           key.startswith("helpfulness-")]
        
        criteria_prefixes = set(key.split('-')[0] for key in criteria_keys)
        
        for prefix in criteria_prefixes:
            metrics["criteria_scores"][prefix] = {}
            prefix_keys = [key for key in criteria_keys if key.startswith(f"{prefix}-")]
            
            # Group by query number
            query_numbers = set(int(key.split('-')[1]) for key in prefix_keys)
            
            for query_num in query_numbers:
                key = f"{prefix}-{query_num}"
                scores = [float(result.get(key, 0)) for result in results if key in result]
                
                if scores:
                    avg_score = sum(scores) / len(scores)
                    metrics["criteria_scores"][prefix][query_num] = {
                        "average": avg_score,
                        "min": min(scores),
                        "max": max(scores),
                        "count": len(scores)
                    }
        
        # Process system preferences
        preference_keys = [key for key in results[0].keys() if key.startswith("preference-")]
        
        for key in preference_keys:
            query_num = int(key.split('-')[1])
            preferences = [result.get(key) for result in results if key in result]
            
            if preferences:
                if query_num not in metrics["system_preferences"]:
                    metrics["system_preferences"][query_num] = {}
                
                for preference in set(preferences):
                    if preference:
                        count = preferences.count(preference)
                        metrics["system_preferences"][query_num][preference] = {
                            "count": count,
                            "percentage": count / len(preferences) * 100
                        }
        
        # Process comments
        comment_keys = [key for key in results[0].keys() if key.startswith("comments-")]
        
        for key in comment_keys:
            query_num = int(key.split('-')[1])
            comments = [result.get(key) for result in results if key in result and result.get(key)]
            
            if comments:
                metrics["comments"].append({
                    "query_num": query_num,
                    "comments": comments
                })
        
        # Calculate overall scores
        overall_scores = {}
        
        for criterion in criteria_prefixes:
            scores = []
            for query_data in metrics["criteria_scores"][criterion].values():
                scores.append(query_data["average"])
            
            if scores:
                overall_scores[criterion] = sum(scores) / len(scores)
        
        metrics["overall_scores"] = overall_scores
        
        # Calculate overall system preference
        if metrics["system_preferences"]:
            system_votes = {"a": 0, "slight-a": 0, "equal": 0, "slight-b": 0, "b": 0}
            total_votes = 0
            
            for query_prefs in metrics["system_preferences"].values():
                for pref, data in query_prefs.items():
                    if pref in system_votes:
                        system_votes[pref] += data["count"]
                        total_votes += data["count"]
            
            if total_votes > 0:
                metrics["overall_system_preference"] = {
                    "counts": system_votes,
                    "percentages": {k: (v / total_votes * 100) for k, v in system_votes.items()},
                    "winner": max(system_votes, key=system_votes.get)
                }
        
        return metrics 